{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### todo list\n",
    "\n",
    "- [ ] Change source name form mnist to \"source\" \n",
    "- [ ] Train each source to each target.\n",
    "- [ ] Add MAPE indicator\n",
    "- [ ] refactor\n",
    "- [ ] TSNE plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MNIST - MNIST-M Domain Adaptation\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus= tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "print(tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPool2D, BatchNormalization, Dropout\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phm_folder= \"../datasets/phm/phm-2012-wt/\"\n",
    "\n",
    "\n",
    "# path_X_train_Bearing1_1 = phm_folder+'X_train_Bearing1_1.npy' \n",
    "# path_X_train_Bearing1_2 = phm_folder+'X_train_Bearing1_2.npy'\n",
    "# path_X_train_Bearing2_1 = phm_folder+'X_train_Bearing2_1.npy' \n",
    "# # path_X_train_Bearing2_2 = phm_folder+'X_train_Bearing2_2.npy' \n",
    "# # path_X_train_Bearing3_1 = phm_folder+'X_train_Bearing3_1.npy' \n",
    "# # path_X_train_Bearing3_2 = phm_folder+'X_train_Bearing3_2.npy' \n",
    "\n",
    "\n",
    "# path_Y_train_Bearing1_1 = phm_folder+'Y_train_Bearing1_1.npy' \n",
    "# path_Y_train_Bearing1_2 = phm_folder+'Y_train_Bearing1_2.npy'\n",
    "# path_Y_train_Bearing2_1 = phm_folder+'Y_train_Bearing2_1.npy' \n",
    "# # path_Y_train_Bearing2_2 = phm_folder+'Y_train_Bearing2_2.npy' \n",
    "# # path_Y_train_Bearing3_1 = phm_folder+'Y_train_Bearing3_1.npy' \n",
    "# # path_Y_train_Bearing3_2 = phm_folder+'Y_train_Bearing3_2.npy' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_Bearing1_1 = np.load(path_X_train_Bearing1_1)\n",
    "# X_train_Bearing1_2 = np.load(path_X_train_Bearing1_2)\n",
    "# X_train_Bearing2_1 = np.load(path_X_train_Bearing2_1)\n",
    "# # X_train_Bearing2_2 = np.load(path_X_train_Bearing2_2)\n",
    "# # X_train_Bearing3_1 = np.load(path_X_train_Bearing3_1)\n",
    "# # X_train_Bearing3_2 = np.load(path_X_train_Bearing3_2)\n",
    "\n",
    "# Y_train_Bearing1_1 = np.load(path_Y_train_Bearing1_1)\n",
    "# Y_train_Bearing1_2 = np.load(path_Y_train_Bearing1_2)\n",
    "# Y_train_Bearing2_1 = np.load(path_Y_train_Bearing2_1)\n",
    "# # Y_train_Bearing2_2 = np.load(path_Y_train_Bearing2_2)\n",
    "# # Y_train_Bearing3_1 = np.load(path_Y_train_Bearing3_1)\n",
    "# # Y_train_Bearing3_2 = np.load(path_Y_train_Bearing3_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csc_folder = \"../datasets/csc/1Y520210304-wt/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_X_train_1Y520210304 = csc_folder+ \"X_train_1Y520210304.npy\"\n",
    "path_Y_train_1Y520210304 = csc_folder+ \"Y_train_1Y520210304.npy\"\n",
    "\n",
    "\n",
    "X_train_1Y520210304 = np.load(path_X_train_1Y520210304)\n",
    "Y_train_1Y520210304 = np.load(path_Y_train_1Y520210304)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8352, 1, 511, 99)\n",
      "(8352,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_1Y520210304.shape)\n",
    "print(Y_train_1Y520210304.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_channel(array_to_move, channel_from, channel_to):\n",
    "    \n",
    "    data = np.moveaxis(array_to_move, channel_from, channel_to)\n",
    "    return data\n",
    "def show_data_shape():\n",
    "    \"\"\" show phm bearing data shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    print( \"X_train_1Y520210304.shape\", X_train_1Y520210304.shape)\n",
    "\n",
    "    \n",
    "    print( \"Y_train_1Y520210304.shape\", Y_train_1Y520210304.shape )\n",
    "\n",
    "    \n",
    "def show_phm_image(phm_img ,mode=\"last\"):\n",
    "    \"\"\" show image according to channel mode\n",
    "    \n",
    "    Args:\n",
    "        phm_img (numpy array): phm image array.\n",
    "        mode (str): channel mode, only accept 'first'(channel_first) or 'last'(channel_last) \n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if mode == \"first\":\n",
    "        plt.imshow(phm_img[0,:,:])\n",
    "    else:\n",
    "        plt.imshow(phm_img[:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## channel first to channel last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Before\n",
      "X_train_1Y520210304.shape (8352, 1, 511, 99)\n",
      "Y_train_1Y520210304.shape (8352,)\n",
      "## After\n",
      "X_train_1Y520210304.shape (8352, 511, 99, 1)\n",
      "Y_train_1Y520210304.shape (8352,)\n"
     ]
    }
   ],
   "source": [
    "print(\"## Before\")\n",
    "show_data_shape()\n",
    "\n",
    "X_train_1Y520210304 = move_channel(X_train_1Y520210304, 1, 3)\n",
    "\n",
    "print(\"## After\")\n",
    "show_data_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "csc_dataset = dict()\n",
    "csc_dataset['1Y520210304'] = {'data': X_train_1Y520210304, 'label':Y_train_1Y520210304}\n",
    "# bearing_dataset['bearing1_1'] = {'data': X_train_Bearing1_1, 'label':Y_train_Bearing1_1}\n",
    "# bearing_dataset['bearing1_2'] = {'data': X_train_Bearing1_2, 'label':Y_train_Bearing1_2}\n",
    "# bearing_dataset['bearing2_1'] = {'data': X_train_Bearing2_1, 'label':Y_train_Bearing2_1}\n",
    "# bearing_dataset['bearing2_2'] = {'data': X_train_Bearing2_2, 'label':Y_train_Bearing2_2}\n",
    "# bearing_dataset['bearing3_1'] = {'data': X_train_Bearing3_1, 'label':Y_train_Bearing3_1}\n",
    "# bearing_dataset['bearing3_2'] = {'data': X_train_Bearing3_2, 'label':Y_train_Bearing3_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANTS\n",
    "# MNIST_M_PATH = './Datasets/MNIST_M/mnistm.h5'\n",
    "\n",
    "BEARING_SOURCE_DATA_NUM = 4500\n",
    "BEARING_TARGET_DATA_NUM = 4500\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "# CHANNELS = 3\n",
    "EPOCH = 15\n",
    "\n",
    "\n",
    "SOURCE_DATA_NAME = '1Y520210304'\n",
    "TARGET_DATA_NAME = '1Y520210304'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_data(data_x, data_y, truncate_num=3000):\n",
    "    \"\"\" make data to certain number\n",
    "    \n",
    "    Args:\n",
    "        data_x (numpy array): phm image array. shape = (image_num, height, width, channel)\n",
    "        data_y (numpy array): phm image label array. shape (image_num,)  \n",
    "        truncate_num (int): truncate the image total number of image. \n",
    "        \n",
    "    Returns:\n",
    "        increased_data_x:  phm image array. shape = (truncate_num, height, width, channel)\n",
    "        increased_data_y:  phm image label array. shape = (truncate_num,)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    repeats_num = int( truncate_num/data_x.shape[0] ) + 1\n",
    "    \n",
    "    _inceresed_data_x = np.repeat(data_x, repeats=repeats_num, axis= 0)\n",
    "    _inceresed_data_y = np.repeat(data_y, repeats=repeats_num, axis= 0)\n",
    "    \n",
    "    return _inceresed_data_x[:truncate_num,:,:], _inceresed_data_y[:truncate_num]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name='bearing1_1'):\n",
    "    \"\"\" load bearing dataset by name\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): csc dataset name\n",
    "\n",
    "    Returns:\n",
    "        numpy array : Bearing image data.\n",
    "        numpy array : Bearing image label according to image.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"[load_dataset] user want to load {} data\".format(dataset_name) )\n",
    "#     dataset_name = dataset_name.lower()\n",
    "    try:\n",
    "        return csc_dataset[dataset_name]['data'], csc_dataset[dataset_name]['label']\n",
    "    except KeyError as e:\n",
    "        print(\"key not found in csc_dataset\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data (Source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load_dataset] user want to load 1Y520210304 data\n"
     ]
    }
   ],
   "source": [
    "mnist_source_x, mnist_source_y = load_dataset(SOURCE_DATA_NAME)\n",
    "\n",
    "mnist_source_x, mnist_source_y = increase_data(mnist_source_x, mnist_source_y, 5000)\n",
    "\n",
    "\n",
    "mnist_train_x = mnist_source_x[:BEARING_SOURCE_DATA_NUM,:,:]\n",
    "mnist_test_x  = mnist_source_x[BEARING_SOURCE_DATA_NUM:(BEARING_SOURCE_DATA_NUM*2),:,:]\n",
    "\n",
    "mnist_train_x, mnist_test_x = mnist_train_x.astype('float32'), mnist_test_x.astype('float32')\n",
    "\n",
    "# mnist_train_x, mnist_test_x = X_train_Bearing1_1, X_train_Bearing1_2\n",
    "# mnist_train_x = mnist_train_x[:BEARING_SOURCE_DATA_NUM,:,:]\n",
    "# mnist_test_x  = mnist_test_x[:BEARING_SOURCE_DATA_NUM,:,:]\n",
    "\n",
    "mnist_train_y = mnist_source_y[:BEARING_SOURCE_DATA_NUM]\n",
    "mnist_test_y  = mnist_source_y[BEARING_SOURCE_DATA_NUM:(BEARING_SOURCE_DATA_NUM*2)]\n",
    "\n",
    "# mnist_train_y = tf.one_hot(mnist_train_y, depth=10)\n",
    "# mnist_test_y = tf.one_hot(mnist_test_y, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 511, 99, 1)\n",
      "(500, 511, 99, 1)\n",
      "(4500,)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "print( mnist_train_x.shape)\n",
    "print(mnist_test_x.shape)\n",
    "print(mnist_train_y.shape)\n",
    "print(mnist_test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load_dataset] user want to load 1Y520210304 data\n"
     ]
    }
   ],
   "source": [
    "mnist_m_target_x, mnist_m_target_y = load_dataset(TARGET_DATA_NAME)\n",
    "\n",
    "mnist_m_target_x, mnist_m_target_y = increase_data(mnist_m_target_x, mnist_m_target_y, 5000)\n",
    "\n",
    "mnist_m_train_x = mnist_m_target_x[:BEARING_TARGET_DATA_NUM,:,:]\n",
    "mnist_m_test_x  = mnist_m_target_x[BEARING_TARGET_DATA_NUM:(BEARING_TARGET_DATA_NUM*2),:,:]\n",
    "\n",
    "mnist_m_train_y = mnist_m_target_y[:BEARING_TARGET_DATA_NUM]\n",
    "mnist_m_test_y  = mnist_m_target_y[BEARING_TARGET_DATA_NUM:(BEARING_TARGET_DATA_NUM*2)]\n",
    "\n",
    "\n",
    "# mnist_m_train_x, mnist_m_test_x = X_train_Bearing2_1, X_train_Bearing2_2\n",
    "# mnist_m_train_x, mnist_m_test_x = mnist_m_train_x.astype('float32'), mnist_m_test_x.astype('float32')\n",
    "\n",
    "# mnist_m_train_x = mnist_m_train_x[:BEARING_SOURCE_DATA_NUM,:,:]\n",
    "# mnist_m_test_x  = mnist_m_test_x[:BEARING_SOURCE_DATA_NUM,:,:]\n",
    "\n",
    "# mnist_m_train_y = Y_train_Bearing2_1[:BEARING_SOURCE_DATA_NUM]\n",
    "# mnist_m_test_y  = Y_train_Bearing2_2[:BEARING_SOURCE_DATA_NUM]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 511, 99, 1)\n",
      "(500, 511, 99, 1)\n",
      "(4500,)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "print(mnist_m_train_x.shape)\n",
    "print(mnist_m_test_x.shape)\n",
    "print(mnist_m_train_y.shape)\n",
    "print(mnist_m_test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Datasets\n",
    "\n",
    "source_dataset = tf.data.Dataset.from_tensor_slices((mnist_train_x, mnist_train_y)).shuffle(1000).batch(BATCH_SIZE*2)\n",
    "da_dataset = tf.data.Dataset.from_tensor_slices((mnist_train_x, mnist_train_y, mnist_m_train_x, mnist_m_train_y)).shuffle(1000).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((mnist_m_test_x, mnist_m_test_y)).shuffle(1000).batch(BATCH_SIZE*2) #Test Dataset over Target Domain\n",
    "test_dataset2 = tf.data.Dataset.from_tensor_slices((mnist_m_train_x, mnist_m_train_y)).shuffle(1000).batch(BATCH_SIZE*2) #Test Dataset over Target (used for training)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Reversal Layer\n",
    "@tf.custom_gradient\n",
    "def gradient_reverse(x, lamda=1.0):\n",
    "    y = tf.identity(x)\n",
    "    \n",
    "    def grad(dy):\n",
    "        return lamda * -dy, None\n",
    "    \n",
    "    return y, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def call(self, x, lamda=1.0):\n",
    "        return gradient_reverse(x, lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANN(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # feature \n",
    "        self.feature_extractor_layer0 = Conv2D(32, kernel_size=(3, 3), activation='relu')\n",
    "        self.feature_extractor_layer1 = BatchNormalization()\n",
    "        self.feature_extractor_layer2 = MaxPool2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.feature_extractor_layer3 = Conv2D(64, kernel_size=(5, 5), activation='relu')\n",
    "        self.feature_extractor_layer4 = Dropout(0.5)\n",
    "        self.feature_extractor_layer5 = BatchNormalization()\n",
    "        self.feature_extractor_layer6 = MaxPool2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        # label\n",
    "        self.label_predictor_layer0 = Dense(100, activation='relu')\n",
    "        self.label_predictor_layer1 = Dense(100, activation='relu')\n",
    "        self.label_predictor_layer2 = Dense(1, activation='sigmoid')\n",
    "        # domain\n",
    "        self.domain_predictor_layer0 = GradientReversalLayer()\n",
    "        self.domain_predictor_layer1 = Dense(100, activation='relu')\n",
    "        self.domain_predictor_layer2 = Dense(2, activation=None)\n",
    "    def call(self, x, train=False, source_train=True, lamda=1.0):\n",
    "\n",
    "        x = self.feature_extractor_layer0(x)\n",
    "        x = self.feature_extractor_layer1(x, training=train)\n",
    "        x = self.feature_extractor_layer2(x)\n",
    "        \n",
    "        x = self.feature_extractor_layer3(x)\n",
    "        x = self.feature_extractor_layer4(x, training=train)\n",
    "        x = self.feature_extractor_layer5(x, training=train)\n",
    "        x = self.feature_extractor_layer6(x)\n",
    "        \n",
    "        # feature = tf.reshape(x, [-1, 64])\n",
    "        feature = tf.keras.layers.Flatten()(x)\n",
    "        if source_train is True:\n",
    "            feature_slice = feature\n",
    "        else:\n",
    "            feature_slice = tf.slice(feature, [0, 0], [feature.shape[0] // 2, -1])\n",
    "        \n",
    "        lp_x = self.label_predictor_layer0(feature_slice)\n",
    "        # print(\"[DANN call] lp_x :\", tf.shape(lp_x))\n",
    "        lp_x = self.label_predictor_layer1(lp_x)\n",
    "        # print(\"[DANN call] lp_x :\", tf.shape(lp_x))\n",
    "        l_logits = self.label_predictor_layer2(lp_x)\n",
    "        # print(\"[DANN call] l_logits :\", tf.shape(l_logits))\n",
    "        if source_train is True:\n",
    "            return l_logits\n",
    "        else:\n",
    "            dp_x = self.domain_predictor_layer0(feature, lamda)    #GradientReversalLayer\n",
    "            dp_x = self.domain_predictor_layer1(dp_x)\n",
    "            d_logits = self.domain_predictor_layer2(dp_x)\n",
    "            \n",
    "            return l_logits, d_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DANN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_rmse(predictions, labels):\n",
    "#     print(\"lose_rmse\")\n",
    "#     print(tf.shape(predictions))\n",
    "#     print(tf.shape(labels))\n",
    "#     return tf.sqrt(tf.reduce_mean((predictions - labels)**2))\n",
    "    return tf.reduce_mean((predictions - labels)**2)\n",
    "\n",
    "def loss_softmax_cross_entropy(input_logits, target_labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=input_logits, labels=target_labels))\n",
    "\n",
    "\n",
    "def get_loss(l_logits, labels, d_logits=None, domain=None):\n",
    "    if d_logits is None:\n",
    "        return loss_rmse(l_logits, labels)\n",
    "    else:\n",
    "        return loss_rmse(l_logits, labels) + loss_softmax_cross_entropy(d_logits, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optimizer = tf.optimizers.SGD()\n",
    "\n",
    "\n",
    "\n",
    "domain_labels = np.vstack([np.tile([1., 0.], [BATCH_SIZE, 1]),\n",
    "                           np.tile([0., 1.], [BATCH_SIZE, 1])])\n",
    "domain_labels = domain_labels.astype('float32')\n",
    "\n",
    "\n",
    "epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "epoch_rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "source_acc = []  # Source Domain Accuracy while Source-only Training\n",
    "source_rmse = []\n",
    "da_acc = []  # Source Domain Accuracy while DA-training\n",
    "da_rmse = []\n",
    "test_acc = []    # Testing Dataset (Target Domain) Accuracy \n",
    "test_rmse = []\n",
    "test2_acc = []   # Target Domain (used for Training) Accuracy\n",
    "test2_rmse = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_source(s_images, s_labels, lamda=1.0):\n",
    "    images = s_images\n",
    "    labels = s_labels\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(images, train=True, source_train=True, lamda=lamda)\n",
    "        \n",
    "#         print(\"[train_step_source] output:\", tf.shape(output))\n",
    "#         print(tf.shape(output))\n",
    "#         print(\"[train_step_source] labes:\", tf.shape(labels))\n",
    "        \n",
    "        model_loss = get_loss(output, labels)\n",
    "        epoch_rmse (output, labels)\n",
    "        epoch_accuracy(output, labels)\n",
    "        \n",
    "    gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\n",
    "    model_optimizer.apply_gradients(zip(gradients_mdan, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_da(s_images, s_labels, t_images=None, t_labels=None, lamda=1.0):\n",
    "    images = tf.concat([s_images, t_images], 0)\n",
    "    labels = s_labels\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(images, train=True, source_train=False, lamda=lamda)\n",
    "        \n",
    "        model_loss = get_loss(output[0], labels, output[1], domain_labels)\n",
    "        epoch_rmse (output[0], labels)\n",
    "#         epoch_accuracy(output[0], labels)\n",
    "        epoch_accuracy(output[1], domain_labels)\n",
    "        \n",
    "    gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\n",
    "    model_optimizer.apply_gradients(zip(gradients_mdan, model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(t_images, t_labels):\n",
    "    images = t_images\n",
    "    labels = t_labels\n",
    "    \n",
    "    output = model(images, train=False, source_train=True)\n",
    "    epoch_rmse (output, labels)\n",
    "#     epoch_accuracy(output, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_mode, epochs=EPOCH):\n",
    "    \n",
    "    if train_mode == 'source':\n",
    "        dataset = source_dataset\n",
    "        train_func = train_step_source\n",
    "        acc_list = source_acc\n",
    "        rmse_list = source_rmse\n",
    "    elif train_mode == 'domain-adaptation':\n",
    "        dataset = da_dataset\n",
    "        train_func = train_step_da\n",
    "        acc_list = da_acc\n",
    "        rmse_list = da_rmse\n",
    "    else:\n",
    "        raise ValueError(\"Unknown training Mode\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        p = float(epoch) / epochs\n",
    "        lamda = 2 / (1 + np.exp(-100 * p, dtype=np.float32)) - 1\n",
    "        lamda = lamda.astype('float32')\n",
    "\n",
    "        for batch in dataset:\n",
    "            train_func(*batch, lamda=lamda)\n",
    "        \n",
    "#         print(\"Training: Epoch {} :\\t Source Accuracy : {:.3%}\".format(epoch, epoch_accuracy.result()), end='  |  ')\n",
    "        print(\"Training: Epoch {} :\\t Source RMSE : {:.3},  Accuracy: {:.3%}\".format(epoch, epoch_rmse.result(), epoch_accuracy.result()))\n",
    "        acc_list.append(epoch_accuracy.result())\n",
    "        rmse_list.append(epoch_rmse.result())\n",
    "        test()\n",
    "        epoch_accuracy.reset_states()\n",
    "        epoch_rmse.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    epoch_accuracy.reset_states()\n",
    "    \n",
    "#     Testing Dataset (Target Domain)\n",
    "    for batch in test_dataset:\n",
    "        test_step(*batch)\n",
    "        \n",
    "#     print(\"Testing Accuracy : {:.3%}\".format(epoch_accuracy.result()), end='  |  ')\n",
    "    print(\"Testing RMASE: {:.3}\".format(epoch_rmse.result()))\n",
    "    test_acc.append(epoch_accuracy.result())\n",
    "    test_rmse.append(epoch_rmse.result())\n",
    "    epoch_accuracy.reset_states()\n",
    "    epoch_rmse.reset_states()\n",
    "#     Target Domain (used for Training)\n",
    "    for batch in test_dataset2:\n",
    "        test_step(*batch)\n",
    "    \n",
    "    print(\"Target domain RMASE: {:.3}\".format(epoch_rmse.result()))\n",
    "#     test2_acc.append(epoch_accuracy.result())\n",
    "    test2_rmse.append(epoch_rmse.result())\n",
    "    epoch_accuracy.reset_states()\n",
    "    epoch_rmse.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 0 :\t Source RMSE : 7.01,  Accuracy: 49.956%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-29786910ae3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# train('source', 5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'domain-adaptation'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-08c75bb74aae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_mode, epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0macc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mrmse_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_rmse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mepoch_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mepoch_rmse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-89dbc77e0272>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     Testing Dataset (Target Domain)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#     print(\"Testing Accuracy : {:.3%}\".format(epoch_accuracy.result()), end='  |  ')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    492\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# train('source', 5)\n",
    "\n",
    "train('domain-adaptation',EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Results\n",
    "x_axis = [i for i in range(0, EPOCH)]\n",
    "\n",
    "plt.plot(x_axis, da_acc, label=\"source accuracy\")\n",
    "# plt.plot(x_axis, test_acc, label=\"testing accuracy\")\n",
    "# plt.plot(x_axis, test2_acc, label=\"target accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "rmse_source = pd.DataFrame(da_rmse).mean()[0]\n",
    "rmse_target = pd.DataFrame(test2_rmse).mean()[0]\n",
    "acc_domain = pd.DataFrame(da_acc).mean()[0]\n",
    "\n",
    "#Plot Results\n",
    "x_axis = [i for i in range(0, EPOCH)]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "label = \"source rmse, with RMSE=\"+str(rmse_source)\n",
    "plt.plot(x_axis, da_rmse, 'o-', label=\"source rmse, with RMSE=\"+str(rmse_source))\n",
    "# plt.plot(x_axis, test_rmse, 'o-', label=\"testing rmse\")\n",
    "plt.plot(x_axis, test2_rmse, 'o-', label=\"target rmse, with RMSE=\"+str(rmse_target))\n",
    "# plt.plot(x_axis, da_acc, label=\"Domain classification accuracy=\"+str(acc_domain))\n",
    "plt.title('Training eopch with RMSE, where Source is Bearing1-2, Target is Bearing1-1.\\n'+\n",
    "         'Domain Classification Accuracy='+str(acc_domain))\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig(SOURCE_DATA_NAME+'_'+TARGET_DATA_NAME+'_dann.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
