{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Goal : check if 2-class classifer work or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### todo list\n",
    "\n",
    "- [ ] Change source name form mnist to \"source\" \n",
    "- [ ] Train each source to each target.\n",
    "- [ ] refactor\n",
    "- [ ] TSNE plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MNIST - MNIST-M Domain Adaptation\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPool2D, BatchNormalization, Dropout\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "phm_folder= \"../../datasets/phm-2012-ewt/\"\n",
    "\n",
    "\n",
    "path_X_train_Bearing1_1 = phm_folder+'X_train_Bearing1_1.npy' \n",
    "path_X_train_Bearing1_2 = phm_folder+'X_train_Bearing1_2.npy'\n",
    "path_X_train_Bearing2_1 = phm_folder+'X_train_Bearing2_1.npy' \n",
    "path_X_train_Bearing2_2 = phm_folder+'X_train_Bearing2_2.npy' \n",
    "path_X_train_Bearing3_1 = phm_folder+'X_train_Bearing3_1.npy' \n",
    "path_X_train_Bearing3_2 = phm_folder+'X_train_Bearing3_2.npy' \n",
    "\n",
    "\n",
    "path_Y_train_Bearing1_1 = phm_folder+'Y_train_Bearing1_1.npy' \n",
    "path_Y_train_Bearing1_2 = phm_folder+'Y_train_Bearing1_2.npy'\n",
    "path_Y_train_Bearing2_1 = phm_folder+'Y_train_Bearing2_1.npy' \n",
    "path_Y_train_Bearing2_2 = phm_folder+'Y_train_Bearing2_2.npy' \n",
    "path_Y_train_Bearing3_1 = phm_folder+'Y_train_Bearing3_1.npy' \n",
    "path_Y_train_Bearing3_2 = phm_folder+'Y_train_Bearing3_2.npy' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_Bearing1_1 = np.load(path_X_train_Bearing1_1)\n",
    "X_train_Bearing1_2 = np.load(path_X_train_Bearing1_2)\n",
    "X_train_Bearing2_1 = np.load(path_X_train_Bearing2_1)\n",
    "X_train_Bearing2_2 = np.load(path_X_train_Bearing2_2)\n",
    "X_train_Bearing3_1 = np.load(path_X_train_Bearing3_1)\n",
    "X_train_Bearing3_2 = np.load(path_X_train_Bearing3_2)\n",
    "\n",
    "Y_train_Bearing1_1 = np.load(path_Y_train_Bearing1_1)\n",
    "Y_train_Bearing1_2 = np.load(path_Y_train_Bearing1_2)\n",
    "Y_train_Bearing2_1 = np.load(path_Y_train_Bearing2_1)\n",
    "Y_train_Bearing2_2 = np.load(path_Y_train_Bearing2_2)\n",
    "Y_train_Bearing3_1 = np.load(path_Y_train_Bearing3_1)\n",
    "Y_train_Bearing3_2 = np.load(path_Y_train_Bearing3_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_channel(array_to_move, channel_from, channel_to):\n",
    "    \n",
    "    data = np.moveaxis(array_to_move, channel_from, channel_to)\n",
    "    return data\n",
    "def show_data_shape():\n",
    "    \"\"\" show phm bearing data shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    print( \"X_train_Bearing1_1.shape\", X_train_Bearing1_1.shape)\n",
    "    print( \"X_train_Bearing1_2.shape\", X_train_Bearing1_2.shape)\n",
    "    print( \"X_train_Bearing2_1.shape\",X_train_Bearing2_1.shape)\n",
    "    print( \"X_train_Bearing2_2.shape\",X_train_Bearing2_2.shape)\n",
    "    print( \"X_train_Bearing3_1.shape\",X_train_Bearing3_1.shape)\n",
    "    print( \"X_train_Bearing3_2.shape\",X_train_Bearing3_2.shape)\n",
    "    \n",
    "    print( \"Y_train_Bearing1_1.shape\", Y_train_Bearing1_1.shape )\n",
    "    print( \"Y_train_Bearing1_2.shape\", Y_train_Bearing1_2.shape )\n",
    "    print( \"Y_train_Bearing2_1.shape\", Y_train_Bearing2_1.shape )\n",
    "    print( \"Y_train_Bearing2_2.shape\", Y_train_Bearing2_2.shape )\n",
    "    print( \"Y_train_Bearing3_1.shape\", Y_train_Bearing3_1.shape )\n",
    "    print( \"Y_train_Bearing3_2.shape\", Y_train_Bearing3_2.shape )\n",
    "    \n",
    "def show_phm_image(phm_img ,mode=\"last\"):\n",
    "    \"\"\" show image according to channel mode\n",
    "    \n",
    "    Args:\n",
    "        phm_img (numpy array): phm image array.\n",
    "        mode (str): channel mode, only accept 'first'(channel_first) or 'last'(channel_last) \n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if mode == \"first\":\n",
    "        plt.imshow(phm_img[0,:,:])\n",
    "    else:\n",
    "        plt.imshow(phm_img[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [1, 0]\n",
    "n_values = np.max(values) + 1\n",
    "a = np.eye(n_values)[values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_numeric_label(numeric_array, split_by=[0.25,0.5,0.75]):\n",
    "    \"\"\" make numeric array to class array\n",
    "    Args:\n",
    "        numeric_array (numpy array): phm image label array. shape = (image_num,)\n",
    "\n",
    "    Returns:\n",
    "        numeric_label_array (numpy array) : numeric label array\n",
    "\n",
    "    Example:\n",
    "        numeric_array = [0.1,0.3,0.4,0.7,1.0]\n",
    "        \n",
    "        numeric_label_array = [0,1,1,2,3]\n",
    "    \n",
    "        a = np.array([0,0.3,0.5,0.7,0.95])\n",
    "        make_numeric_label(a, [0.25,0.5,0.9])\n",
    "        >>> array([0, 1, 1, 2, 3])\n",
    "    \n",
    "        a = np.array([0,0.3,0.5,0.7,0.95])\n",
    "        make_numeric_label(a, [0.5])\n",
    "        >>> array([0, 0, 0, 1, 1])\n",
    "    \n",
    "    \"\"\"\n",
    "    select_filter = [numeric_array <= split_by[0]]\n",
    "    if len(split_by) == 1:\n",
    "        pass\n",
    "    else:\n",
    "        select_filter.extend([(split_by[i]< numeric_array) & (numeric_array <= split_by[i+1]) for i in range(len(split_by)-1)])\n",
    "    \n",
    "    select_filter.append(numeric_array > split_by[-1])\n",
    "    \n",
    "    \n",
    "    label = list(range(len(select_filter)))\n",
    "    numeric_label_array = np.select(select_filter, label)\n",
    "    \n",
    "    return numeric_label_array\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_to_onehot(label_array):\n",
    "    \"\"\" make label array to one-hot\n",
    "\n",
    "    Args:\n",
    "        label_array (numpy array): phm image label array. shape = (image_num,)\n",
    "            example:  [0,1,0,2,3,4] \n",
    "\n",
    "    Returns:\n",
    "        one_hot_label_array (numpy array) : one hot encoding array\n",
    "\n",
    "    Example:\n",
    "        label_array =  [0,1,0,2,3,4] \n",
    "        return [[1,0,0,0,0],[0,1,0,0,0],[1,0,0,0,0],....]\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "#     n_values = n_values.astype(int)\n",
    "    n_values = np.max(label_array) + 1\n",
    "    one_hot_label_array = np.eye(n_values)[label_array]\n",
    "    one_hot_label_array = one_hot_label_array.astype(np.int)\n",
    "    \n",
    "    \n",
    "#     u, counts = np.unique(label_array, return_counts=True)\n",
    "#     print(u)\n",
    "#     print(counts)\n",
    "    \n",
    "    return one_hot_label_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## channel first to channel last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Before\n",
      "X_train_Bearing1_1.shape (2803, 1, 128, 128)\n",
      "X_train_Bearing1_2.shape (871, 1, 128, 128)\n",
      "X_train_Bearing2_1.shape (911, 1, 128, 128)\n",
      "X_train_Bearing2_2.shape (797, 1, 128, 128)\n",
      "X_train_Bearing3_1.shape (515, 1, 128, 128)\n",
      "X_train_Bearing3_2.shape (1637, 1, 128, 128)\n",
      "Y_train_Bearing1_1.shape (2803,)\n",
      "Y_train_Bearing1_2.shape (871,)\n",
      "Y_train_Bearing2_1.shape (911,)\n",
      "Y_train_Bearing2_2.shape (797,)\n",
      "Y_train_Bearing3_1.shape (515,)\n",
      "Y_train_Bearing3_2.shape (1637,)\n",
      "## After\n",
      "X_train_Bearing1_1.shape (2803, 128, 128, 1)\n",
      "X_train_Bearing1_2.shape (871, 128, 128, 1)\n",
      "X_train_Bearing2_1.shape (911, 128, 128, 1)\n",
      "X_train_Bearing2_2.shape (797, 128, 128, 1)\n",
      "X_train_Bearing3_1.shape (515, 128, 128, 1)\n",
      "X_train_Bearing3_2.shape (1637, 128, 128, 1)\n",
      "Y_train_Bearing1_1.shape (2803,)\n",
      "Y_train_Bearing1_2.shape (871,)\n",
      "Y_train_Bearing2_1.shape (911,)\n",
      "Y_train_Bearing2_2.shape (797,)\n",
      "Y_train_Bearing3_1.shape (515,)\n",
      "Y_train_Bearing3_2.shape (1637,)\n"
     ]
    }
   ],
   "source": [
    "print(\"## Before\")\n",
    "show_data_shape()\n",
    "\n",
    "X_train_Bearing1_1 = move_channel(X_train_Bearing1_1, 1, 3)\n",
    "X_train_Bearing1_2 = move_channel(X_train_Bearing1_2, 1, 3)\n",
    "X_train_Bearing2_1 = move_channel(X_train_Bearing2_1, 1, 3)\n",
    "X_train_Bearing2_2 = move_channel(X_train_Bearing2_2, 1, 3)\n",
    "X_train_Bearing3_1 = move_channel(X_train_Bearing3_1, 1, 3)\n",
    "X_train_Bearing3_2 = move_channel(X_train_Bearing3_2, 1, 3)\n",
    "print(\"## After\")\n",
    "show_data_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make label(indicator) to be one-hot by threshold\n",
    "- threshild = [0.5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2803, 2)\n",
      "(871, 2)\n",
      "(911, 2)\n",
      "(797, 2)\n",
      "(515, 2)\n",
      "(1637, 2)\n"
     ]
    }
   ],
   "source": [
    "a = make_numeric_label(Y_train_Bearing1_1, split_by=[0.5])\n",
    "b = numeric_to_onehot(a)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "onehot_Y_train_Bearing1_1 = numeric_to_onehot(make_numeric_label(Y_train_Bearing1_1, split_by=[0.5]))\n",
    "onehot_Y_train_Bearing1_2 = numeric_to_onehot(make_numeric_label(Y_train_Bearing1_2, split_by=[0.5]))\n",
    "onehot_Y_train_Bearing2_1 = numeric_to_onehot(make_numeric_label(Y_train_Bearing2_1, split_by=[0.5]))\n",
    "onehot_Y_train_Bearing2_2 = numeric_to_onehot(make_numeric_label(Y_train_Bearing2_2, split_by=[0.5]))\n",
    "onehot_Y_train_Bearing3_1 = numeric_to_onehot(make_numeric_label(Y_train_Bearing3_1, split_by=[0.5]))\n",
    "onehot_Y_train_Bearing3_2 = numeric_to_onehot(make_numeric_label(Y_train_Bearing3_2, split_by=[0.5]))\n",
    "\n",
    "print(onehot_Y_train_Bearing1_1.shape)\n",
    "print(onehot_Y_train_Bearing1_2.shape)\n",
    "print(onehot_Y_train_Bearing2_1.shape)\n",
    "print(onehot_Y_train_Bearing2_2.shape)\n",
    "print(onehot_Y_train_Bearing3_1.shape)\n",
    "print(onehot_Y_train_Bearing3_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bearing_dataset = dict()\n",
    "bearing_dataset['bearing1_1'] = {'data': X_train_Bearing1_1, 'label':onehot_Y_train_Bearing1_1}\n",
    "bearing_dataset['bearing1_2'] = {'data': X_train_Bearing1_2, 'label':onehot_Y_train_Bearing1_2}\n",
    "bearing_dataset['bearing2_1'] = {'data': X_train_Bearing2_1, 'label':onehot_Y_train_Bearing2_1}\n",
    "bearing_dataset['bearing2_2'] = {'data': X_train_Bearing2_2, 'label':onehot_Y_train_Bearing2_2}\n",
    "bearing_dataset['bearing3_1'] = {'data': X_train_Bearing3_1, 'label':onehot_Y_train_Bearing3_1}\n",
    "bearing_dataset['bearing3_2'] = {'data': X_train_Bearing3_2, 'label':onehot_Y_train_Bearing3_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANTS\n",
    "# MNIST_M_PATH = './Datasets/MNIST_M/mnistm.h5'\n",
    "\n",
    "BEARING_SOURCE_DATA_NUM = 1500\n",
    "BEARING_TARGET_DATA_NUM = 1500\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "# CHANNELS = 3\n",
    "EPOCH = 20\n",
    "\n",
    "\n",
    "SOURCE_DATA_NAME = 'Bearing1_1'\n",
    "TARGET_DATA_NAME = 'Bearing1_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_data(data_x, data_y, truncate_num=3000):\n",
    "    \"\"\" make data to certain number\n",
    "    \n",
    "    Args:\n",
    "        data_x (numpy array): phm image array. shape = (image_num, height, width, channel)\n",
    "        data_y (numpy array): phm image label array. shape (image_num,)  \n",
    "        truncate_num (int): truncate the image total number of image. \n",
    "        \n",
    "    Returns:\n",
    "        increased_data_x:  phm image array. shape = (truncate_num, height, width, channel)\n",
    "        increased_data_y:  phm image label array. shape = (truncate_num,)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    repeats_num = int( truncate_num/data_x.shape[0] ) + 1\n",
    "    \n",
    "    _inceresed_data_x = np.repeat(data_x, repeats=repeats_num, axis= 0)\n",
    "    _inceresed_data_y = np.repeat(data_y, repeats=repeats_num, axis= 0)\n",
    "    \n",
    "    return _inceresed_data_x[:truncate_num,:,:], _inceresed_data_y[:truncate_num]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name='bearing1_1'):\n",
    "    \"\"\" load bearing dataset by name\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): Bearing dataset name\n",
    "\n",
    "    Returns:\n",
    "        numpy array : Bearing image data.\n",
    "        numpy array : Bearing image label according to image.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"[load_dataset] user want to load {} data\".format(dataset_name) )\n",
    "    dataset_name = dataset_name.lower()\n",
    "    try:\n",
    "        return bearing_dataset[dataset_name]['data'], bearing_dataset[dataset_name]['label']\n",
    "    except KeyError as e:\n",
    "        print(\"key not found in bearing_dataset\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PHM Data (Source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load_dataset] user want to load Bearing1_1 data\n"
     ]
    }
   ],
   "source": [
    "mnist_source_x, mnist_source_y = load_dataset(SOURCE_DATA_NAME)\n",
    "\n",
    "mnist_source_x, mnist_source_y = increase_data(mnist_source_x, mnist_source_y, 5000)\n",
    "\n",
    "\n",
    "# mnist_train_x = mnist_source_x[:BEARING_SOURCE_DATA_NUM,:,:]\n",
    "# mnist_test_x  = mnist_source_x[BEARING_SOURCE_DATA_NUM:(BEARING_SOURCE_DATA_NUM*2),:,:]\n",
    "# mnist_train_x, mnist_test_x = mnist_train_x.astype('float32'), mnist_test_x.astype('float32')\n",
    "\n",
    "# mnist_train_y = mnist_source_y[:BEARING_SOURCE_DATA_NUM,:]\n",
    "# mnist_test_y  = mnist_source_y[BEARING_SOURCE_DATA_NUM:(BEARING_SOURCE_DATA_NUM*2),:]\n",
    "\n",
    "\n",
    "mnist_train_x, mnist_test_x, mnist_train_y, mnist_test_y = train_test_split(\n",
    "    mnist_source_x, mnist_source_y, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "mnist_train_x, mnist_test_x = mnist_train_x.astype('float32'), mnist_test_x.astype('float32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# a_x = np.array([[1,11],[2,12],[3,13],[4,14],[5,15],[6,16]])\n",
    "# a_y = np.array([21,22,23,24,25,26])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     a_x, a_y, test_size=0.33, random_state=42\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 128, 128, 1)\n",
      "(2500, 128, 128, 1)\n",
      "(2500, 2)\n",
      "(2500, 2)\n"
     ]
    }
   ],
   "source": [
    "print( mnist_train_x.shape)\n",
    "print(mnist_test_x.shape)\n",
    "print(mnist_train_y.shape)\n",
    "print(mnist_test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PHM Data(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load_dataset] user want to load Bearing1_2 data\n"
     ]
    }
   ],
   "source": [
    "mnist_m_target_x, mnist_m_target_y = load_dataset(TARGET_DATA_NAME)\n",
    "\n",
    "mnist_m_target_x, mnist_m_target_y = increase_data(mnist_m_target_x, mnist_m_target_y, 5000)\n",
    "\n",
    "# mnist_m_train_x = mnist_m_target_x[:BEARING_TARGET_DATA_NUM,:,:]\n",
    "# mnist_m_test_x  = mnist_m_target_x[BEARING_TARGET_DATA_NUM:(BEARING_TARGET_DATA_NUM*2),:,:]\n",
    "\n",
    "# mnist_m_train_y = mnist_m_target_y[:BEARING_TARGET_DATA_NUM,:]\n",
    "# mnist_m_test_y  = mnist_m_target_y[BEARING_TARGET_DATA_NUM:(BEARING_TARGET_DATA_NUM*2),:]\n",
    "\n",
    "\n",
    "mnist_m_train_x, mnist_m_test_x, mnist_m_train_y, mnist_m_test_y = train_test_split(\n",
    "    mnist_m_target_x, mnist_m_target_y, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "mnist_m_train_x, mnist_m_test_x = mnist_m_train_x.astype('float32'), mnist_m_test_x.astype('float32')\n",
    "\n",
    "\n",
    "# mnist_m_train_x, mnist_m_test_x = X_train_Bearing2_1, X_train_Bearing2_2\n",
    "# mnist_m_train_x, mnist_m_test_x = mnist_m_train_x.astype('float32'), mnist_m_test_x.astype('float32')\n",
    "\n",
    "# mnist_m_train_x = mnist_m_train_x[:BEARING_SOURCE_DATA_NUM,:,:]\n",
    "# mnist_m_test_x  = mnist_m_test_x[:BEARING_SOURCE_DATA_NUM,:,:]\n",
    "\n",
    "# mnist_m_train_y = Y_train_Bearing2_1[:BEARING_SOURCE_DATA_NUM]\n",
    "# mnist_m_test_y  = Y_train_Bearing2_2[:BEARING_SOURCE_DATA_NUM]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 128, 128, 1)\n",
      "(2500, 128, 128, 1)\n",
      "(2500, 2)\n",
      "(2500, 2)\n"
     ]
    }
   ],
   "source": [
    "print(mnist_m_train_x.shape)\n",
    "print(mnist_m_test_x.shape)\n",
    "print(mnist_m_train_y.shape)\n",
    "print(mnist_m_test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Datasets\n",
    "\n",
    "source_dataset = tf.data.Dataset.from_tensor_slices((mnist_train_x, mnist_train_y)).shuffle(1000).batch(BATCH_SIZE*2)\n",
    "da_dataset = tf.data.Dataset.from_tensor_slices((mnist_train_x, mnist_train_y, mnist_m_train_x, mnist_m_train_y)).shuffle(1000).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((mnist_test_x, mnist_test_y)).shuffle(1000).batch(BATCH_SIZE*2) #Test Dataset over Target Domain\n",
    "# test_dataset2 = tf.data.Dataset.from_tensor_slices((mnist_m_train_x, mnist_m_train_y)).shuffle(1000).batch(BATCH_SIZE*2) #Test Dataset over Target (used for training)\n",
    "test_dataset2 = tf.data.Dataset.from_tensor_slices((mnist_m_train_x, mnist_m_train_y)).shuffle(1000).batch(BATCH_SIZE*2) #Test Dataset over Target (used for training)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Reversal Layer\n",
    "@tf.custom_gradient\n",
    "def gradient_reverse(x, lamda=1.0):\n",
    "    y = tf.identity(x)\n",
    "    \n",
    "    def grad(dy):\n",
    "        return lamda * -dy, None\n",
    "    \n",
    "    return y, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def call(self, x, lamda=1.0):\n",
    "        return gradient_reverse(x, lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANN(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # feature \n",
    "        self.feature_extractor_layer0 = Conv2D(32, kernel_size=(3, 3), activation='relu')\n",
    "        self.feature_extractor_layer1 = BatchNormalization()\n",
    "        self.feature_extractor_layer2 = MaxPool2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.feature_extractor_layer3 = Conv2D(64, kernel_size=(5, 5), activation='relu')\n",
    "        self.feature_extractor_layer4 = Dropout(0.5)\n",
    "        self.feature_extractor_layer5 = BatchNormalization()\n",
    "        self.feature_extractor_layer6 = MaxPool2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        # label\n",
    "        self.label_predictor_layer0 = Dense(100, activation='relu')\n",
    "        self.label_predictor_layer1 = Dense(100, activation='relu')\n",
    "        self.label_predictor_layer2 = Dense(2, activation='softmax')\n",
    "        # domain\n",
    "        self.domain_predictor_layer0 = GradientReversalLayer()\n",
    "        self.domain_predictor_layer1 = Dense(100, activation='relu')\n",
    "        self.domain_predictor_layer2 = Dense(2, activation=None)\n",
    "    def call(self, x, train=False, source_train=True, lamda=1.0):\n",
    "\n",
    "        x = self.feature_extractor_layer0(x)\n",
    "        x = self.feature_extractor_layer1(x, training=train)\n",
    "        x = self.feature_extractor_layer2(x)\n",
    "        \n",
    "        x = self.feature_extractor_layer3(x)\n",
    "        x = self.feature_extractor_layer4(x, training=train)\n",
    "        x = self.feature_extractor_layer5(x, training=train)\n",
    "        x = self.feature_extractor_layer6(x)\n",
    "        \n",
    "        # feature = tf.reshape(x, [-1, 64])\n",
    "        feature = tf.keras.layers.Flatten()(x)\n",
    "        if source_train is True:\n",
    "            feature_slice = feature\n",
    "        else:\n",
    "            feature_slice = tf.slice(feature, [0, 0], [feature.shape[0] // 2, -1])\n",
    "        \n",
    "        lp_x = self.label_predictor_layer0(feature_slice)\n",
    "        # print(\"[DANN call] lp_x :\", tf.shape(lp_x))\n",
    "        lp_x = self.label_predictor_layer1(lp_x)\n",
    "        # print(\"[DANN call] lp_x :\", tf.shape(lp_x))\n",
    "        l_logits = self.label_predictor_layer2(lp_x)\n",
    "        # print(\"[DANN call] l_logits :\", tf.shape(l_logits))\n",
    "        if source_train is True:\n",
    "            return l_logits\n",
    "        else:\n",
    "            dp_x = self.domain_predictor_layer0(feature, lamda)    #GradientReversalLayer\n",
    "            dp_x = self.domain_predictor_layer1(dp_x)\n",
    "            d_logits = self.domain_predictor_layer2(dp_x)\n",
    "            \n",
    "            return l_logits, d_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DANN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_rmse(predictions, labels):\n",
    "#     print(\"lose_rmse\")\n",
    "#     print(tf.shape(predictions))\n",
    "#     print(tf.shape(labels))\n",
    "#     return tf.sqrt(tf.reduce_mean((predictions - labels)**2))\n",
    "    return tf.reduce_mean((predictions - labels)**2)\n",
    "\n",
    "def loss_softmax_cross_entropy(input_logits, target_labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=input_logits, labels=target_labels))\n",
    "\n",
    "\n",
    "def get_loss(l_logits, labels, d_logits=None, domain=None):\n",
    "    if d_logits is None:\n",
    "#         return loss_rmse(l_logits, labels)\n",
    "        return loss_softmax_cross_entropy(l_logits, labels)\n",
    "    else:\n",
    "        return loss_softmax_cross_entropy(l_logits, labels) + loss_softmax_cross_entropy(d_logits, domain)\n",
    "#         return loss_rmse(l_logits, labels) + loss_softmax_cross_entropy(d_logits, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optimizer = tf.optimizers.SGD()\n",
    "\n",
    "\n",
    "\n",
    "domain_labels = np.vstack([np.tile([1., 0.], [BATCH_SIZE, 1]),\n",
    "                           np.tile([0., 1.], [BATCH_SIZE, 1])])\n",
    "domain_labels = domain_labels.astype('float32')\n",
    "\n",
    "\n",
    "epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "epoch_domain_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "epoch_crossentropy = tf.keras.metrics.CategoricalCrossentropy()\n",
    "epoch_rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "\n",
    "source_acc = []  # Source Domain Accuracy while Source-only Training\n",
    "source_rmse = []\n",
    "source_cross_entropy = []\n",
    "da_acc = []  # Source Domain Accuracy while DA-training\n",
    "# da_rmse = []\n",
    "da_cross_entropy = []\n",
    "test_acc = []    # Testing Dataset (Target Domain) Accuracy \n",
    "# test_rmse = []\n",
    "test_cross_entropy = []\n",
    "\n",
    "test2_acc = []   # Target Domain (used for Training) Accuracy\n",
    "# test2_rmse = []\n",
    "test2_cross_entropy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_source(s_images, s_labels, lamda=1.0):\n",
    "    images = s_images\n",
    "    labels = s_labels\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(images, train=True, source_train=True, lamda=lamda)\n",
    "        \n",
    "#         print(\"[train_step_source] output:\", tf.shape(output))\n",
    "#         print(tf.shape(output))\n",
    "#         print(\"[train_step_source] labes:\", tf.shape(labels))\n",
    "        \n",
    "        model_loss = get_loss(output, labels)\n",
    "#         epoch_rmse (output, labels)\n",
    "        epoch_crossentropy(output, labels)\n",
    "        epoch_accuracy(output, labels)\n",
    "        \n",
    "    gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\n",
    "    model_optimizer.apply_gradients(zip(gradients_mdan, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_da(s_images, s_labels, t_images=None, t_labels=None, lamda=1.0):\n",
    "    images = tf.concat([s_images, t_images], 0)\n",
    "    labels = s_labels\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(images, train=True, source_train=False, lamda=lamda)\n",
    "        \n",
    "        model_loss = get_loss(output[0], labels, output[1], domain_labels)\n",
    "        epoch_crossentropy(output[0], labels)\n",
    "        epoch_accuracy(output[0], labels)\n",
    "        epoch_domain_accuracy(output[1], domain_labels)\n",
    "        \n",
    "    gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\n",
    "    model_optimizer.apply_gradients(zip(gradients_mdan, model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(t_images, t_labels):\n",
    "    images = t_images\n",
    "    labels = t_labels\n",
    "    \n",
    "    output = model(images, train=False, source_train=True)\n",
    "    epoch_rmse (output, labels)\n",
    "    epoch_accuracy(output, labels)\n",
    "    epoch_crossentropy(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_mode, epochs=EPOCH):\n",
    "    \n",
    "    if train_mode == 'source':\n",
    "        dataset = source_dataset\n",
    "        train_func = train_step_source\n",
    "        acc_list = source_acc\n",
    "#         rmse_list = source_rmse\n",
    "        rmse_list = source_cross_entropy\n",
    "    elif train_mode == 'domain-adaptation':\n",
    "        dataset = da_dataset\n",
    "        train_func = train_step_da\n",
    "        acc_list = da_acc\n",
    "        rmse_list = da_cross_entropy\n",
    "#         rmse_list = da_rmse\n",
    "    else:\n",
    "        raise ValueError(\"Unknown training Mode\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        p = float(epoch) / epochs\n",
    "        lamda = 2 / (1 + np.exp(-100 * p, dtype=np.float32)) - 1\n",
    "        lamda = lamda.astype('float32')\n",
    "\n",
    "        for batch in dataset:\n",
    "            train_func(*batch, lamda=lamda)\n",
    "        \n",
    "#         print(\"Training: Epoch {} :\\t Source Accuracy : {:.3%}\".format(epoch, epoch_accuracy.result()), end='  |  ')\n",
    "        print(\"Training: Epoch {} :\\t Source cross entropy : {:.3},  Accuracy: {:.3%}, domain Accuracy:{:.3%}\".format(epoch, epoch_crossentropy.result(), epoch_accuracy.result(), epoch_domain_accuracy.result()))\n",
    "        acc_list.append(epoch_accuracy.result())\n",
    "        rmse_list.append(epoch_rmse.result())\n",
    "        test()\n",
    "        epoch_accuracy.reset_states()\n",
    "        epoch_domain_accuracy.reset_states()\n",
    "        epoch_crossentropy.reset_states()\n",
    "        epoch_rmse.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    epoch_accuracy.reset_states()\n",
    "    \n",
    "#     Testing Dataset (Target Domain)\n",
    "    for batch in test_dataset:\n",
    "        test_step(*batch)\n",
    "        \n",
    "#     print(\"Testing Accuracy : {:.3%}\".format(epoch_accuracy.result()), end='  |  ')\n",
    "    print(\"Testing label cross entropy: {:.3}, acc:{:.3}\".format(epoch_crossentropy.result(), epoch_accuracy.result()))\n",
    "    test_acc.append(epoch_accuracy.result())\n",
    "#     test_rmse.append(epoch_rmse.result())\n",
    "    test_cross_entropy.append(epoch_crossentropy.result())\n",
    "    \n",
    "    epoch_accuracy.reset_states()\n",
    "    epoch_crossentropy.reset_states()\n",
    "    epoch_rmse.reset_states()\n",
    "#     Target Domain (used for Training)\n",
    "    for batch in test_dataset2:\n",
    "        test_step(*batch)\n",
    "    \n",
    "    print(\"Target label cross entropy: {:.3}, acc:{:.3}\".format(epoch_crossentropy.result(), epoch_accuracy.result() ))\n",
    "    test2_acc.append(epoch_accuracy.result())\n",
    "    test2_cross_entropy.append(epoch_crossentropy.result())\n",
    "    \n",
    "    epoch_accuracy.reset_states()\n",
    "    epoch_domain_accuracy.reset_states()\n",
    "    epoch_crossentropy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE:Bearing1_1, TARGET:Bearing1_2\n",
      "Training: Epoch 0 :\t Source cross entropy : 2.01,  Accuracy: 87.520%, domain Accuracy:95.280%\n",
      "Testing label cross entropy: 1.9, acc:0.894\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_rmse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-011a4afcc0a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# train('source', 5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SOURCE:{source_name}, TARGET:{target_name}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSOURCE_DATA_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTARGET_DATA_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'domain-adaptation'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-af5cef5bfe80>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_mode, epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0macc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mrmse_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_rmse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mepoch_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mepoch_domain_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-3330d24755fb>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtest_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#     test_rmse.append(epoch_rmse.result())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtest_rmse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_crossentropy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mepoch_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_rmse' is not defined"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# train('source', 5)\n",
    "print(\"SOURCE:{source_name}, TARGET:{target_name}\".format(source_name=SOURCE_DATA_NAME, target_name=TARGET_DATA_NAME))\n",
    "train('domain-adaptation',EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Results\n",
    "x_axis = [i for i in range(0, EPOCH)]\n",
    "\n",
    "plt.plot(x_axis, da_acc, label=\"source accuracy\")\n",
    "plt.plot(x_axis, test_acc, label=\"testing accuracy\")\n",
    "plt.plot(x_axis, test2_acc, label=\"target accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Results\n",
    "x_axis = [i for i in range(0, EPOCH)]\n",
    "\n",
    "plt.plot(x_axis, da_cross_entropy, label=\"source cross e\")\n",
    "plt.plot(x_axis, test_cross_entropy, label=\"testing cross e\")\n",
    "plt.plot(x_axis, test2_cross_entropy, label=\"target cross e\")\n",
    "# plt.plot(x_axis, da_acc, label=\"source accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
